{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages to be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports raw data\n",
    "housing_import = pd.read_csv('trimmed_data.csv', index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning out NaN values remaining\n",
    "housing_import = housing_import[housing_import.MasVnrType.isnull() == False]\n",
    "housing = housing_import.drop('MiscFeature', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My interpretation of the get dummies function is that it took each categorical variable and added a variable to the dataframe for each category and populated that variable with a binary to capture datapoints from that category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_dummies = pd.get_dummies(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Target and Feature Arrays\n",
    "# .values returns numpy array instead of dataframe\n",
    "X = housing_dummies.drop('SalePrice', axis=1).values\n",
    "y = housing_dummies.SalePrice.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsure as what I need as far as reshaping goes\n",
    "X.shape\n",
    "y.shape\n",
    "\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for performance metrics\n",
    "d = {'R_Squared': [], 'RMSE': [], 'MAPE': []}\n",
    "metrics = pd.DataFrame(data=d)\n",
    "metrics.index.name = 'Model'\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LinearRegression Object\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Predictions based on fit\n",
    "y_pred_test = reg.predict(X_test)\n",
    "y_pred_train = reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics for training set\n",
    "print(\"R^2: {}\".format(reg.score(X_train, y_train)))\n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "error = (y_train-y_pred_train)/y_train * 100\n",
    "mape = abs(error).mean()\n",
    "print('Mean Absolute Percent Error: {}'.format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metrics to the metrics dataframe\n",
    "d = {'R_Squared': [reg.score(X_train, y_train)], 'RMSE': [rmse], 'MAPE': [mape]}\n",
    "temp = pd.DataFrame(data=d, index=['Linear_Train'])\n",
    "temp.index.name = 'Model'\n",
    "metrics = metrics.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics for test set\n",
    "print(\"R^2: {}\".format(reg.score(X_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "error = (y_test-y_pred_test)/y_test * 100\n",
    "mape = abs(error).mean()\n",
    "print('Mean Absolute Percent Error: {}'.format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metrics to the metrics dataframe\n",
    "d = {'R_Squared': [reg.score(X_test, y_test)], 'RMSE': [rmse], 'MAPE': [mape]}\n",
    "temp = pd.DataFrame(data=d, index=['Linear_Test'])\n",
    "temp.index.name = 'Model'\n",
    "metrics = metrics.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Ridge Regression\n",
    "\n",
    "# Train test split -- Random state 42 should produce same split as above\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)\n",
    "\n",
    "# Creating Ridge object, fitting, and predicting. Normalize = true ensures all variables are on the same scale\n",
    "ridge = Ridge(alpha=0.1, normalize = True)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred_test = ridge.predict(X_test)\n",
    "ridge_pred_train = ridge.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for train set\n",
    "print(\"R^2: {}\".format(ridge.score(X_train, y_train)))\n",
    "rmse = np.sqrt(mean_squared_error(y_train, ridge_pred_train))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "error = (y_train-ridge_pred_train)/y_train * 100\n",
    "mape = abs(error).mean()\n",
    "print('Mean Absolute Percent Error: {}'.format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metrics to the metrics dataframe\n",
    "d = {'R_Squared': [ridge.score(X_train, y_train)], 'RMSE': [rmse], 'MAPE': [mape]}\n",
    "temp = pd.DataFrame(data=d, index=['Ridge_Train'])\n",
    "temp.index.name = 'Model'\n",
    "metrics = metrics.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics for test set\n",
    "print(\"R^2: {}\".format(ridge.score(X_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, ridge_pred_test))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "error = (y_test-ridge_pred_test)/y_test * 100\n",
    "mape = abs(error).mean()\n",
    "print('Mean Absolute Percent Error: {}'.format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metrics to the metrics dataframe\n",
    "d = {'R_Squared': [ridge.score(X_test, y_test)], 'RMSE': [rmse], 'MAPE': [mape]}\n",
    "temp = pd.DataFrame(data=d, index=['Ridge_Test'])\n",
    "temp.index.name = 'Model'\n",
    "metrics = metrics.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Lasso Regression\n",
    "\n",
    "# Train test split -- Random state 42 should produce same split as above\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)\n",
    "\n",
    "lasso = Lasso(alpha = 0.1, normalize = True)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_pred_test = lasso.predict(X_test)\n",
    "lasso_pred_train = lasso.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics for train set\n",
    "print(\"R^2: {}\".format(lasso.score(X_train, y_train)))\n",
    "rmse = np.sqrt(mean_squared_error(y_train, lasso_pred_train))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "error = (y_train-lasso_pred_train)/y_train * 100\n",
    "mape = abs(error).mean()\n",
    "print('Mean Absolute Percent Error: {}'.format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metrics to the metrics dataframe\n",
    "d = {'R_Squared': [lasso.score(X_train, y_train)], 'RMSE': [rmse], 'MAPE': [mape]}\n",
    "temp = pd.DataFrame(data=d, index=['Lasso_Train'])\n",
    "temp.index.name = 'Model'\n",
    "metrics = metrics.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics for test set\n",
    "print(\"R^2: {}\".format(lasso.score(X_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, lasso_pred_test))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "error = (y_test-lasso_pred_test)/y_test * 100\n",
    "mape = abs(error).mean()\n",
    "print('Mean Absolute Percent Error: {}'.format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metrics to the metrics dataframe\n",
    "d = {'R_Squared': [lasso.score(X_test, y_test)], 'RMSE': [rmse], 'MAPE': [mape]}\n",
    "temp = pd.DataFrame(data=d, index=['Lasso_Test'])\n",
    "temp.index.name = 'Model'\n",
    "metrics = metrics.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs actual scatterplot\n",
    "plt.scatter(ridge_pred_test, y_test, alpha=0.5, s=4)\n",
    "plt.title('Predicted vs. Actual Sale price')\n",
    "plt.xlabel('Predicted Sale Price')\n",
    "plt.ylabel('Actual Sale Price')\n",
    "plt.plot(list(range(y_test.max())),list(range(y_test.max())), c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs residuals\n",
    "residuals = ridge_pred_test-y_test\n",
    "plt.scatter(ridge_pred_test, residuals, alpha = 0.5, s=4)\n",
    "plt.plot(list(range(y_test.max())),[0]*y_test.max(), c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Residuals\n",
    "mean = np.mean(residuals)\n",
    "std_dev = np.std(residuals)\n",
    "\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.axvline(mean, color='r')\n",
    "plt.axvline(mean + std_dev, color='r', linestyle='--')\n",
    "plt.axvline(mean + 2* std_dev, color='r', linestyle='-.')\n",
    "plt.axvline(mean + 3* std_dev, color='r', linestyle=':')\n",
    "plt.axvline(mean - std_dev, color='r', linestyle='--')\n",
    "plt.axvline(mean - 2* std_dev, color='r', linestyle='-.')\n",
    "plt.axvline(mean - 3* std_dev, color='r', linestyle=':')\n",
    "plt.title('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals[residuals > mean + 3* std_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals[residuals < mean - 3* std_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the hyperparameter grid\n",
    "alpha_space = list(np.arange(11)/10)\n",
    "param_grid = {'alpha': alpha_space}\n",
    "\n",
    "# Instantiate a ridge regression classifier: ridge\n",
    "ridge = Ridge()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "ridge_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(ridge_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(ridge_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Ridge Regression with different alpha based on grid search CV results\n",
    "\n",
    "# Train test split -- Random state 42 should produce same split as above\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)\n",
    "\n",
    "# Creating Ridge object, fitting, and predicting. Normalize = true ensures all variables are on the same scale\n",
    "ridge = Ridge(alpha=1, normalize = True)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred_test = ridge.predict(X_test)\n",
    "ridge_pred_train = ridge.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for train set\n",
    "print(\"R^2: {}\".format(ridge.score(X_train, y_train)))\n",
    "rmse = np.sqrt(mean_squared_error(y_train, ridge_pred_train))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "error = (y_train-ridge_pred_train)/y_train * 100\n",
    "mape = abs(error).mean()\n",
    "print('Mean Absolute Percent Error: {}'.format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metrics to the metrics dataframe\n",
    "d = {'R_Squared': [ridge.score(X_train, y_train)], 'RMSE': [rmse], 'MAPE': [mape]}\n",
    "temp = pd.DataFrame(data=d, index=['Ridge_Train_a=1'])\n",
    "temp.index.name = 'Model'\n",
    "metrics = metrics.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics for test set\n",
    "print(\"R^2: {}\".format(ridge.score(X_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, ridge_pred_test))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "error = (y_test-ridge_pred_test)/y_test * 100\n",
    "mape = abs(error).mean()\n",
    "print('Mean Absolute Percent Error: {}'.format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metrics to the metrics dataframe\n",
    "d = {'R_Squared': [ridge.score(X_test, y_test)], 'RMSE': [rmse], 'MAPE': [mape]}\n",
    "temp = pd.DataFrame(data=d, index=['Ridge_Test_a=1'])\n",
    "temp.index.name = 'Model'\n",
    "metrics = metrics.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerunning graphics with different ridge model\n",
    "\n",
    "# Predicted vs actual scatterplot\n",
    "plt.scatter(ridge_pred_test, y_test, alpha=0.5, s=4)\n",
    "plt.title('Predicted vs. Actual Sale price')\n",
    "plt.xlabel('Predicted Sale Price')\n",
    "plt.ylabel('Actual Sale Price')\n",
    "plt.plot(list(range(y_test.max())),list(range(y_test.max())), c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs residuals\n",
    "residuals = ridge_pred_test-y_test\n",
    "residuals_train = ridge_pred_train-y_train\n",
    "plt.scatter(ridge_pred_test, residuals, alpha = 0.5, s=4)\n",
    "plt.plot(list(range(y_test.max())),[0]*y_test.max(), c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogram of Residuals\n",
    "mean = np.mean(residuals)\n",
    "std_dev = np.std(residuals)\n",
    "\n",
    "mean_train = np.mean(residuals_train)\n",
    "std_dev_train = np.std(residuals_train)\n",
    "\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.axvline(mean, color='r')\n",
    "plt.axvline(mean + std_dev, color='r', linestyle='--')\n",
    "plt.axvline(mean + 2* std_dev, color='r', linestyle='-.')\n",
    "plt.axvline(mean + 3* std_dev, color='r', linestyle=':')\n",
    "plt.axvline(mean - std_dev, color='r', linestyle='--')\n",
    "plt.axvline(mean - 2* std_dev, color='r', linestyle='-.')\n",
    "plt.axvline(mean - 3* std_dev, color='r', linestyle=':')\n",
    "plt.title('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "residuals[residuals > mean + 3* std_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals[residuals < mean - 3* std_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A set of all indicies of high outlier points\n",
    "high_residuals = np.where(abs(residuals) > mean + 3* std_dev)\n",
    "high_residuals_train = np.where(abs(residuals_train) > mean_train + 3*std_dev_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question for removing outliers. Should I be finding and removing outliers from the train set or the test set? Both? \n",
    "\n",
    "\n",
    "# Dropping Outliers\n",
    "X_test_no_outliers = np.delete(X_test, high_residuals[0], axis=0)\n",
    "y_test_no_outliers = np.delete(y_test, high_residuals[0], axis=0)\n",
    "X_train_no_outliers = np.delete(X_train, high_residuals_train[0], axis=0)\n",
    "y_train_no_outliers = np.delete(y_train, high_residuals_train[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_no_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_no_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_residuals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_residuals_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_no_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Ridge Regression with outliers removed\n",
    "\n",
    "# Creating Ridge object, fitting, and predicting. Normalize = true ensures all variables are on the same scale\n",
    "ridge = Ridge(alpha=1, normalize = True)\n",
    "ridge.fit(X_train_no_outliers, y_train_no_outliers)\n",
    "ridge_pred_no_test = ridge.predict(X_test_no_outliers)\n",
    "ridge_pred_no_train = ridge.predict(X_train_no_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for train set\n",
    "print(\"R^2: {}\".format(ridge.score(X_train_no_outliers, y_train_no_outliers)))\n",
    "rmse = np.sqrt(mean_squared_error(y_train_no_outliers, ridge_pred_no_train))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "error = (y_train_no_outliers-ridge_pred_no_train)/y_train_no_outliers * 100\n",
    "mape = abs(error).mean()\n",
    "print('Mean Absolute Percent Error: {}'.format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metrics to the metrics dataframe\n",
    "d = {'R_Squared': [ridge.score(X_train_no_outliers, y_train_no_outliers)], 'RMSE': [rmse], 'MAPE': [mape]}\n",
    "temp = pd.DataFrame(data=d, index=['Ridge_Train_No_Outliers'])\n",
    "temp.index.name = 'Model'\n",
    "metrics = metrics.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics for test set\n",
    "print(\"R^2: {}\".format(ridge.score(X_test_no_outliers, y_test_no_outliers)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test_no_outliers, ridge_pred_no_test))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "error = (y_test_no_outliers-ridge_pred_no_test)/y_test_no_outliers * 100\n",
    "mape = abs(error).mean()\n",
    "print('Mean Absolute Percent Error: {}'.format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metrics to the metrics dataframe\n",
    "d = {'R_Squared': [ridge.score(X_test_no_outliers, y_test_no_outliers)], 'RMSE': [rmse], 'MAPE': [mape]}\n",
    "temp = pd.DataFrame(data=d, index=['Ridge_Test_No_Outliers'])\n",
    "temp.index.name = 'Model'\n",
    "metrics = metrics.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerunning graphics with No_outliers model\n",
    "\n",
    "# Predicted vs actual scatterplot\n",
    "plt.scatter(ridge_pred_no_test, y_test_no_outliers, alpha=0.5, s=4)\n",
    "plt.title('Predicted vs. Actual Sale price')\n",
    "plt.xlabel('Predicted Sale Price')\n",
    "plt.ylabel('Actual Sale Price')\n",
    "plt.plot(list(range(y_test_no_outliers.max())),list(range(y_test_no_outliers.max())), c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs residuals\n",
    "residuals = ridge_pred_no_test-y_test_no_outliers\n",
    "residuals_train = ridge_pred_no_train-y_train_no_outliers\n",
    "plt.scatter(ridge_pred_no_test, residuals, alpha = 0.5, s=4)\n",
    "plt.plot(list(range(y_test_no_outliers.max())),[0]*y_test_no_outliers.max(), c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Histogram of Residuals\n",
    "\n",
    "mean = np.mean(residuals)\n",
    "std_dev = np.std(residuals)\n",
    "\n",
    "mean_train = np.mean(residuals_train)\n",
    "std_dev_train = np.std(residuals_train)\n",
    "\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.axvline(mean, color='r')\n",
    "plt.axvline(mean + std_dev, color='r', linestyle='--')\n",
    "plt.axvline(mean + 2* std_dev, color='r', linestyle='-.')\n",
    "plt.axvline(mean + 3* std_dev, color='r', linestyle=':')\n",
    "plt.axvline(mean - std_dev, color='r', linestyle='--')\n",
    "plt.axvline(mean - 2* std_dev, color='r', linestyle='-.')\n",
    "plt.axvline(mean - 3* std_dev, color='r', linestyle=':')\n",
    "plt.title('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
